---
title: "ML Supervisado - Obligatorio"
author: "De Polsi - del Palacio - González"
date: "12/8/2019"
geometry: "left=3cm,right=3cm,top=2cm,bottom=2cm"
output:
  pdf_document:
    df_print: kable
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#Librerias
library(knitr)
library(rio)
library(dplyr)
library(tidyverse)
library(lubridate)
library(smbinning)
library(DT)
library(ggplot2)
library(fastDummies)
library(gbm)
library(kableExtra)
library(DT)
library(gridExtra)
library(DataExplorer)
library(pander)
library(caret)
library(tree)
library(rpart)
library(rpart.plot)
library(randomForest)

#Importacion de datos
#setwd("C:/Users/emili/Desktop/ORT Analitica/Modelos Supervisados/Obligatorio")
#datos<- read.csv('CreditScoring.csv', stringsAsFactors = TRUE, encoding = "Latin-1")

#Directorios - Javier
base.dir <- "C:/Users/emili/Desktop/ORT Analitica/Modelos Supervisados/Obligatorio"
 
# data.dir <- file.path(base.dir, "datos")
# report.dir <- file.path(base.dir, "reportes")
# graf.dir <- file.path(base.dir, "graficos")
# models.dir <- file.path(base.dir, "modelos")
#  
setwd(base.dir)
 
#Importacion de datos
 
datos<- import('CreditScoring.csv', stringsAsFactors = TRUE, encoding = "Latin-1")


#Verificacion de los datos incluidos
dim(datos)


# Funcion Tbl
tblFun <- function(x){
    tbl <- table(x)
    res <- cbind(tbl,round(prop.table(tbl)*100,2))
    colnames(res) <- c('Count','Percentage')
    res
}

```

## Exploración Inicial
  
### Estructura de los datos y estadísticas descriptivas.

```{r echo=FALSE}
panderOptions('round', 2)
kable(head(datos), format = "latex")  %>%
          kable_styling(latex_options = "scale_down")
str(datos)
kable(summary(datos[,-1]), format = "latex")  %>%
          kable_styling(latex_options = "scale_down")
```
  
  
  
### Amálisis Univariado de Variable indpeneidente "Default".

```{r echo=FALSE, fig.width=6, fig.height=4}
kable(tblFun(datos$Default))
```

```{r echo=FALSE, fig.width=5, fig.height=4}
par(mfrow = c(1,1))
ggplot(datos, aes(Default))+geom_bar(aes(fill=Default))+
  scale_fill_manual(values=c("gree n4", "red"))+ggtitle("Proporcion Default") 
```
  
  
  
### Análsis vibariado de las variables del dataset contra la variable "Default".  

```{r echo=FALSE}
grid.arrange(
datos %>% ggplot(aes(Edad))+geom_bar(aes(fill=Default), position = "fill")+
  scale_fill_manual(values=c("gree n4", "red")),
datos %>% ggplot(aes(Sexo))+geom_bar(aes(fill=Default), position = "fill")+
  scale_fill_manual(values=c("gree n4", "red")),
datos %>% ggplot(aes(NED))+geom_bar(aes(fill=Default), position = "fill")+
  scale_fill_manual(values=c("gree n4", "red")),
datos %>% ggplot(aes(NumBancos))+geom_bar(aes(fill=Default), position = "fill")+
  scale_fill_manual(values=c("gree n4", "red")),
datos %>% ggplot(aes(NumFinanc))+geom_bar(aes(fill=Default), position = "fill")+
  scale_fill_manual(values=c("gree n4", "red")),
datos %>% ggplot(aes(NED))+geom_bar(aes(fill=Default), position = "fill")+
  scale_fill_manual(values=c("gree n4", "red")),
datos %>% ggplot(aes(C6M))+geom_bar(aes(fill=Default), position = "fill")+
  scale_fill_manual(values=c("gree n4", "red")),
ncol=2)
```

```{r echo=FALSE}
plot_boxplot(datos[,-c(1,5,6)], by='Default', nrow = 3L, ncol = 2L)
```
  
  
  
### Creación de Variables

```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# FEATURE ENGINEERING
#BINARIZACIÓN DEFAULT
BinAux <- dummy_cols(.data = datos, select_columns = "Default")
datos <- mutate(datos, Default = BinAux$Default_M)

# C6M
datos <- datos %>% mutate(Rangos_C6M = ifelse(C6M == '1A'|C6M == '1C','AL_DIA',
                                              ifelse(C6M == '2A'|C6M == '2B'|C6M == '3','ATR_MODERADO',
                                                     ifelse(C6M == '4'|C6M == '5','ATR_ALTO','N/A'))))

datos$Rangos_C6M <- factor(datos$Rangos_C6M, order = TRUE, 
                                    levels = c("AL_DIA", "ATR_MODERADO", "ATR_ALTO"))
# EDAD
datos <- datos %>% mutate(Rangos_EDAD = ifelse(Edad < 25,'MENOR_25',
                                              ifelse(Edad >= 25 & Edad < 35,'ENTRE_25_34',
                                                     ifelse(Edad >= 35 & Edad < 45,'ENTRE_35_44',
                                                            ifelse(Edad >= 45 & Edad < 55,'ENTRE_45_54',
                                                                   ifelse(Edad >= 55,'MAYOR_55','N/A'))))))
datos$Rangos_EDAD <- factor(datos$Rangos_EDAD, order = TRUE, 
                           levels = c("MENOR_25", "ENTRE_25_34", "ENTRE_35_44", "ENTRE_45_54", "MAYOR_55"))
# NUMBANCOS
datos <- datos %>% mutate(SOLO_BANCOS = ifelse(NumBancos > 0 & NumFinanc == 0,1,0))
# NUMBFINANC
datos <- datos %>% mutate(SOLO_FINANC = ifelse(NumFinanc > 0 & NumBancos == 0,1,0))
# CONT / INGRESO
datos <- datos %>% mutate(CONTsobreINGRESO = Cont/Ingreso)
# COtización Dic 2019 - 1 USD - $38
# VIGEN_PESOS
datos <- datos %>% mutate(VIGEN_PESOS = Vigen*38)
# CONT / VIGEN_PESOS
datos <- datos %>% mutate(CONTsobreVIGEN_PESOS = Cont/VIGEN_PESOS)

datos$PI <- as.factor(datos$PI)
datos$Rangos_C6M <- as.factor(datos$Rangos_C6M)
datos$Rangos_EDAD <- as.factor(datos$Rangos_EDAD)
datos$SOLO_BANCOS <- as.factor(datos$SOLO_BANCOS)
datos$SOLO_FINANC <- as.factor(datos$SOLO_FINANC)

correlaciones <- sort(sapply(datos[,-c(1:4,8,10,14:18)], cor, y=datos$Default), decreasing = TRUE)

sort_abs <- function(x, na.last = TRUE, decreasing = TRUE) {
  x[order(abs(x), na.last = na.last, decreasing = decreasing)] 
}
correlacionorden <- sort_abs(correlaciones)

# Modelo Boosting - Influencia Relativa
set.seed(612)
boost <- gbm(Default ~ ., data = datos[,-c(1,2,8)] , distribution = "bernoulli", n.trees = 100, 
            shrinkage = 0.1, interaction.depth = 1, bag.fraction = 0.5, train.fraction = 0.5,
            n.minobsinnode = 10, cv.folds = 5, keep.data = TRUE)

# Partición de la muestra en train y test
n <- nrow(datos)
train.x <- datos[1:(n*0.8),-c(1,2,8,14)]
train.y <- datos$Default[1:(n*0.8)]
test.x <- datos[((n*0.8)+1):n,-c(1,2,8,14)]
test.y <- datos$Default[((n*0.8)+1):n]
df.train <- data.frame ("y" = train.y, train.x)
df.train$y<- as.factor(df.train$y)

#Árboles de Decision
modelo_dt <- rpart(y ~ . ,data = df.train, method = "class", minbucket = 10 , maxdepth = 6)
y_hat_dt <- ifelse (predict(modelo_dt , as.data.frame(test.x))[,2]>0.5,1,0)
tabla_dt <- table(y_hat_dt, test.y)
er_dt <- 1- sum(diag(tabla_dt))/sum(tabla_dt)

#Random Forest

mod_rf <- randomForest( y  ~ . , data = df.train, mtry = 6, ntree = 400, importance = TRUE)
y_hat_rf <- predict(mod_rf, test.x)
tabla_rf <- table(y_hat_rf, test.y)
er_rf <- 1- sum(diag(tabla_rf))/sum(tabla_rf)
importanceOrdenRF <- sort_abs(mod_rf$importance[,4])

#Boosting
grid <- expand.grid(n.trees = 400, interaction.depth= 1, 
                    shrinkage=0.1, n.minobsinnode=10)
ctrl <- trainControl(method = "cv",number = 5)

unwantedoutput <- capture.output(mod_GBM <- train(y~.,data = df.train, distribution="bernoulli",
                                                   method = "gbm", trControl = ctrl, tuneGrid = grid))
conf<- confusionMatrix(mod_GBM)

#Random Forest óptimo
      
mod_rf_op <- randomForest( y  ~ . , data = df.train, mtry = 7, ntree = 400, importance = TRUE)
y_hat_rf_op <- predict(mod_rf_op, test.x)
tabla_rf_op <- table(y_hat_rf_op, test.y)
er_rf_op <- 1- sum(diag(tabla_rf_op))/sum(tabla_rf_op)

#Boosting Optimo
gridopt <- expand.grid(n.trees = 400, interaction.depth= 3, 
                    shrinkage=0.1, n.minobsinnode=10)
ctrlopt <- trainControl(method = "cv",number = 5)

unwantedoutputopt <- capture.output(mod_GBMopt <- train(y~.,data = df.train, distribution="bernoulli",
                                                  method = "gbm", trControl = ctrlopt, tuneGrid = gridopt))

y_hat_boost_op <- predict(mod_GBMopt, test.x)
tabla_boost_op <- table(y_hat_boost_op, test.y)
er_boost_op <- 1- sum(diag(tabla_boost_op))/sum(tabla_boost_op)

```
  
  
  
A partir de la exploración inicial y la visualización gráfica, se realizaron agrupaciones en variables como *Edad* y *Calificación en el historial de pago*. Se binarizan las variables *Cantidad de Bancos* y *Cantidad de Financieras no Bancarias*. También se realizan operaciones aritmeticas con variables continuas como *Ingreso*, *Contingencias* y *Créditos vigentes*. 

```{r echo=FALSE}
panderOptions('round', 2)
kable(summary(datos[,-c(1:13)]))%>%
          kable_styling(latex_options = "scale_down")
```
  
### Importancia de las variables

Para pbservar la importancia de las variables, en primer lugar se analiza la **correlación** entre las variables continuas y la variable dependiente.

```{r echo=FALSE}
kable(correlacionorden)
```

En segunda instancias se corre un modelo **Boosting**, sin realizar optimización de hiperparametros, en todo el dataset para chequear la *influencia relativa* de cada predictor en el descenso promedio del error de clasificación.

```{r echo=FALSE} 
summary(boost)
```

Con los datos obtenidos se pdrían tomar desiciones con las variables a utilizar en la estimación de los modelos. En el caso de este trabajo decidimos mantener todos los predictores.
  
## Estimacion de Modelos

### Árbol de clasificación. Modelo Base

Se opta por utilizar un **Árbol de Clasificación** como modelo base. Otra buena opción hubiera sido tomar como base un modelo de *Regresión Loística*. Por la velocidad y simplicidad con la que se logran las estimaciones este modelo es la base de comparción para la elección de modelos más complejos que se estiman posteriormente. 

```{r echo=FALSE} 
printcp(modelo_dt)
rpart.plot(modelo_dt)
```
  
  
  
### Random Forest

El primer modelo estimado para competir con el *Modelo Base* corresponde a un **Random Forest** sin optimización de hiperparamatros. Se estima con **400** *arboles* y la cantidad aleatoria de *predictores* seleccionados para las divisiones igual a **6**. 

Se presenta la *importancia de las variables* para el modelo estimado así como los resultados de la matriz de confusión en *train* set.

```{r echo=FALSE}
 print(mod_rf)

 kable(importanceOrdenRF)
```
  
  
  
### Boosting

Por último se estima un modelo de **Boosting** sin optimización de hiperparametros. Se utiliza una cantidad de *arboles* igual a **400**, Un *coeficiente de aprendizaje* para cada arbol de **0.1** y una cantidad de *interacciones entre variables de cada arbol* igual a **1**   

Se presenta la *importancia de las variables* para el modelo estimado así como los resultados en *train* set.

```{r echo=FALSE}
(conf$table)/100 *40000

summary(mod_GBM)
```
  
  
  
## Principales Hiperparametros a optimizar

### Arbol de Clasificación

En Árboles de Clasificación, la importancia de optimizar hiperparametros es no caer en *overfitting* que reduzca la capacidad predictiva.
Los principales hiperparametros a optimizar en el la *muestra de test* son el *tamaño del arbol*, en *r* *maxdepht* o el *proceso de podado*, representado por *cp* en *r*.
  
  
  
### Random Forest

El principal hiperparametro a optimizar en Random forest será el *número de predictores* utilizado en cada división (en *r mtry*).
Otros hiperparametros del modelo pueden ser el número mínimo de *observaciones* que deben tener los *nodos terminales*, en *r nodesisze* y el *Número de Arboles* que se ajustan en el modelo (en *r ntree*). El número de arboles y el crecimiento que estos pueden tener no son considerados *hiperparametros críticos*. Estos es así ya que en los modelos donde se aplica muestreo con *boostrapping* se elimina el riego de caer en *overfitting*.
  
  
  
### Boosting

En boosting identificamos 3 hiperparametros críticos a optimizar:
La *cantidad de árboles*, en *r n.trees*. Vale aclarar que boosting no hace uso de muestreo *bootstrapping*, por lo que cada árbol construido depende en gran medida de los árboles previos. Esto genera un riesgo alto *overfitting* para un númro grande de árboles.  
Por otro lado se tiene el parámetro de aprendizaje, en *r shrinkage*, el cual controla el ritmo al que aprende el modelo de cada árbol. Con coeficientes de aprendizaje muy bajos será necesario un mayor número de árboles para obtener buenos resultados.
Finalmente tenemoe la cantidad de *interacciones* entre variables de cada árbol pérmitidas.
  
  
  
## Optimización de hiperparámetros

Por razones de economía de tiempo se presentan los códigos utilizados para la optimización de cada uno de los modelos y se explicitan los resultados obtenidos.
  
  
  
### Árbol de clasificación

Hiperparámetros seleccionado a optimizar en Árbol de Clasificación para el proceso de podado es: *cp*  
En función de las optimizaciones realizadas, los valores son:  
**cp** : 0.000 -  0.005  - 0.010  
**Minimo Error_dt** : 0.1839  

```{r echo=TRUE, eval=FALSE, tidy=TRUE}

n <- nrow(datos)
train.x = datos[1:(n*0.8),-c(1,2,8,14)]
train.y = datos$Default[1:(n*0.8)]

test.x = datos[((n*0.8)+1):n,-c(1,2,8,14)]
test.y = datos$Default[((n*0.8)+1):n]

df.train = data.frame ("y" = train.y, train.x)


#Prune el modelo
modelo_dtp = prune(modelo_dt) # modelo_dt es el modelo base especificado en el punto 2  
alfas <- seq(0,0.1, by = 0.005)
error_dt = c()
for(i in alfas){
  modelo_dtp = prune(modelo_dt, cp = i)
  y_hat_dt = ifelse (predict(modelo_dtp , as.data.frame(test.x))[,2]>0.5,1,0)
  tabla_dt = table(y_hat_dt, test.y)
  er_dt = 1- sum(diag(tabla_dt))/sum(tabla_dt)
  error_dt <- c(error_dt, er_dt)
}

#[1] 0.1839 0.1839 0.1839 0.1949 0.1949 0.1949 0.1949 0.2028 0.2028 0.2028 0.2028 
#   0.2028 0.2028 0.2028
#[15] 0.2028 0.2028 0.2028 0.2028 0.2028 0.2028 0.2028

# Minimo Error_dt : 0.1839
# cp 0.000 0.005 0.010
```

En este caso la mejor *accuracy* obtenida es la misma que el para el *podado* utilizado en el *modelo base* por lo que no se volvió a correr el modelo. 
  
  

### Random Forest

Hiperparámetros a optimizar para el Random Forest son: *ntree* y *mtry*.  
En función las optimizaciones realizadas, los valores son:  
**ntree** = 400  
**mtry** = 7  
**Error rate** 0.1251  
**Accuracy** 0.8749  

```{r echo=TRUE, eval=FALSE, tidy=TRUE}

n <- nrow(datos)
train.x = datos[1:(n*0.8),-c(1,2,8,14)]
train.y = datos$Default[1:(n*0.8)]

test.x = datos[((n*0.8)+1):n,-c(1,2,8,14)]
test.y = datos$Default[((n*0.8)+1):n]

df.train = data.frame ("y" = train.y, train.x)

# Hiperparametros a Optimizar mtry y ntree
vectorTrees <- seq(100,400, by = 100)
datosRF <- c("nt"=  NA, "mtry" = NA, "er" = NA, "accuracy" = NA)

for(i in vectorTrees){
  for(mtry in 1:17){
    rf = randomForest( y  ~ . , data = df.train, mtry = mtry, ntree = i, importance = TRUE)
    iter_y_hat_bag = predict(rf, test.x)
    iter_tabla_bag = table(iter_y_hat_bag, test.y)
    iter_er_bag = 1- sum(diag(iter_tabla_bag))/sum(iter_tabla_bag)
    
    resAux <- c(i, mtry, iter_er_bag, (1 - iter_er_bag))
    datosRF <- rbind(datosRF, resAux)
    cat(mtry," ") #printing the output to the console
    
  }
}

MinDatosRF <- min(datosRF[,3])

OrdenDatosRF <- sort_abs(datosRF[,3])

# Better Acuuracy is:
# Árboles  400  MTRY  7  - Error 0.1251  Accuracy 0.8749


```
  
  

### Boosting opción 1

Hiperparámetros seleccionado a optimizar en Boosting para el proceso 2 son: *interaction.depth* , *shrinkage*, *n.trees*.  
En función de las optimizaciones realizadas, los valores son:  
**n.trees** : 400   
**interaction.depth** : 3  
**shrinkage** : 0.100  

```{r echo=TRUE, eval=FALSE, tidy=TRUE}

grid <- expand.grid(n.trees = seq(100,400, by=100), interaction.depth=c(1:3),
                    shrinkage=c(0.001,0.005,0.01,0.05,0.1), n.minobsinnode = 10)

ctrl <- trainControl(method = "cv",number = 5, allowParallel = FALSE)

set.seed(124) #for reproducability

unwantedoutput <- capture.output(GBMModel <- train(y~.,data = df.train,
                                                   method = "gbm", trControl = ctrl, tuneGrid = grid))

GBMModel$results %>% as.data.frame() %>% 
  select(1:2, 4:5) %>% 
  filter(Accuracy > 0.85) %>% 
  arrange(desc(Accuracy))

#Better  n.trees = 400  , interaction.depth = 3 , shrinkage =  0.100 
# Accuracy = 0.882675

```
  
  

### Boosting opción 2

Hiperparámetros seleccionado a optimizar en Boosting para el proceso 2 son: *interaction.depth* , *shrinkage*, *n.trees*.  
En función de las optimizaciones realizadas, los valores son:  
**n.trees** : 900   
**interaction.depth** : 4  
**shrinkage** : 0.100  

```{r echo=TRUE, eval=FALSE, tidy=TRUE}

set.seed(612)
vectorLambda <- c(0.001, 0.005, 0.01, 0.05, 0.1)
vectorArboles <- seq(100,1000, by = 200)
vectorD <- c(1:5)

datosDF <- c("nt"=  NA, "d" = NA, "lambda" = NA, "er" = NA)

for(i in vectorArboles){
  for (j in vectorD){
    for (h in vectorLambda){
      
      modgbm <- gbm(Default ~ ., data = datos[,-c(1,2,8)], n.trees = i, interaction.depth = j, 
                    shrinkage = h, bag.fraction = 0.5, train.fraction = 0.8, cv.folds = 5 , 
                    n.minobsinnode = 10 ,keep.data = TRUE)
      
      er <- mean(modgbm$cv.error)
      resAux <- c(i, j, h, er)
      datosDF <- rbind(datosDF, resAux)
    }
    
  }
  cat(i," ") #printing the output to the console
}

OrdenDatosDF <- sort_abs(datosDF[,4])
MinDatosDF <- min(datosDF[,4])

#Better  n.trees = 900  , interaction.depth = 4 , shrinkage =  0.100 
# cv.error = 0.5565731

```

    
  
El hecho que el óptimo se encontrara en mayor número de árboles de la iteración, tanto en Random Forest como en Boosting (donde además la optimización del hiperparámetro de aprendizaje es el más alto iterado) indica que se debería haber permitido iterar con mayor número de árboles.  
  
    
## Error de clasificación y Modelo elegido

### Árbol de clasificación

```{r echo=FALSE}
tabla_dt
er_dt
```
  
  
  
### Random Forest

```{r echo=FALSE}
tabla_rf_op
er_rf_op
```
  
  
  
### Boosting

```{r echo=FALSE}
tabla_boost_op
er_boost_op
```

  

En función del error de clasificación nos quedamos con el modelo de **Boosting**. Es importante señalar que el modelo no solo es el que presenta menor error de clasificación sino que además es el que consigue mejor clasificación en los casos de éxito (**Default**).

  
  
  
